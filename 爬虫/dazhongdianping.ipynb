{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 设置请求头，模拟浏览器访问\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "    \"Cookie\": \"_lxsdk_cuid=193b544a72fc8-026d202573c394-26011851-144000-193b544a72fc8; _lxsdk=193b544a72fc8-026d202573c394-26011851-144000-193b544a72fc8; _hc.v=65c3183f-14b4-7d2c-f46d-88a102276353.1733912996; s_ViewType=10; ua=dpuser_3514064540; ctu=5f9373f1a224b576a5e42830c308791e18489f3daf5c11028df005d64e30d897; cityid=98; fspop=test; _lx_utm=utm_source%3Dgoogle%26utm_medium%3Dorganic; Hm_lvt_602b80cf8079ae6591966cc70a3940e7=1738237847; HMACCOUNT=C2A9454786D9FDA0; WEBDFPID=uuvwz3z02v21506wy331v8x002z4yzyu806y455wz479795861zw4zzu-1738324397486-1733913006752GAGAESCfd79fef3d01d5e9aadc18ccd4d0c95073349; qruuid=3925bdfd-9a2a-4889-8e9d-fedcbd7ee6cd; dplet=c8386e43e25d0e573efc85e16aca3283; dper=0202f9201b66bc08b350edc319a4e79085385273c0869259c6d1c20c94e197f5683e4cd47a31dee4f83dc1f64edfb46f3823dcb4fcf160187cba0000000084260000554d59cb4b5065980715aaaea152764698aeb77fbea91b3960a02744b64f07feda310a9d28b1d01402253053c3042c91; ll=7fd06e815b796be3df069dec7836c3df; cy=9; cye=chongqing; __CACHE@is_login=true; __CACHE@referer=https://www.dianping.com/search/keyword/9/0_%E9%A4%90%E5%8E%85; logan_custom_report=; logan_session_token=9a4c725il7uil5t3fzri; Hm_lpvt_602b80cf8079ae6591966cc70a3940e7=1738313420; _lxsdk_s=194bb8da6c1-ef8-041-ed%7C%7C42\"\n",
    "}\n",
    "\n",
    "# 目标URL模板\n",
    "base_url = \"https://www.dianping.com/chongqing/ch10/p\"\n",
    "\n",
    "# 爬取的页数\n",
    "total_pages = 50\n",
    "\n",
    "# 存储数据的列表\n",
    "data_list = []\n",
    "\n",
    "# 解析评分\n",
    "def parse_rating(stars):\n",
    "    if \"star_50\" in stars:\n",
    "        return 5.0\n",
    "    elif \"star_45\" in stars:\n",
    "        return 4.5\n",
    "    elif \"star_40\" in stars:\n",
    "        return 4.0\n",
    "    elif \"star_35\" in stars:\n",
    "        return 3.5\n",
    "    elif \"star_30\" in stars:\n",
    "        return 3.0\n",
    "    elif \"star_25\" in stars:\n",
    "        return 2.5\n",
    "    elif \"star_20\" in stars:\n",
    "        return 2.0\n",
    "    elif \"star_15\" in stars:\n",
    "        return 1.5\n",
    "    elif \"star_10\" in stars:\n",
    "        return 1.0\n",
    "    elif \"star_05\" in stars:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# 遍历多个页面\n",
    "for page in range(1, total_pages + 1):\n",
    "    url = f\"{base_url}{page}\"\n",
    "    print(f\"正在爬取：{url}\")\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"第 {page} 页请求失败，状态码：{response.status_code}\")\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # 解析所有店铺信息\n",
    "    shop_list = soup.find_all(\"li\", class_=\"\")\n",
    "\n",
    "    for shop in shop_list:\n",
    "        try:\n",
    "            # 店铺名称\n",
    "            name_tag = shop.find(\"h4\")\n",
    "            name = name_tag.text.strip() if name_tag else \"未知\"\n",
    "\n",
    "            # 详情链接\n",
    "            link_tag = shop.find(\"a\", attrs={\"data-click-name\": \"shop_title_click\"})\n",
    "            link = link_tag[\"href\"] if link_tag else \"未知\"\n",
    "\n",
    "            # 图片链接\n",
    "            img_tag = shop.find(\"img\")\n",
    "            img_url = img_tag[\"data-src\"] if img_tag and \"data-src\" in img_tag.attrs else \"未知\"\n",
    "\n",
    "            # 评分\n",
    "            star_tag = shop.find(\"span\", class_=\"star\")\n",
    "            star_class = star_tag[\"class\"] if star_tag else []\n",
    "            rating = parse_rating(\" \".join(star_class))\n",
    "\n",
    "            # 评价数量\n",
    "            review_tag = shop.find(\"a\", class_=\"review-num\")\n",
    "            reviews = review_tag.b.text.strip() if review_tag else \"未知\"\n",
    "\n",
    "            # 人均消费\n",
    "            price_tag = shop.find(\"a\", class_=\"mean-price\")\n",
    "            price = price_tag.b.text.strip().replace(\"￥\", \"\") if price_tag else \"未知\"\n",
    "\n",
    "            # 地理位置\n",
    "            location_tags = shop.find_all(\"span\", class_=\"tag\")\n",
    "            location = \" | \".join(tag.text.strip() for tag in location_tags) if location_tags else \"未知\"\n",
    "\n",
    "            # 推荐菜品\n",
    "            recommend_tag = shop.find(\"div\", class_=\"recommend\")\n",
    "            if recommend_tag:\n",
    "                dishes = [a.text.strip() for a in recommend_tag.find_all(\"a\")]\n",
    "                recommended_dishes = \"，\".join(dishes)\n",
    "            else:\n",
    "                recommended_dishes = \"无推荐菜\"\n",
    "\n",
    "            # 存储数据\n",
    "            data_list.append([name, link, img_url, rating, reviews, price, location, recommended_dishes])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"解析店铺信息时发生错误：{e}\")\n",
    "\n",
    "    # 避免被封，增加随机延迟\n",
    "    time.sleep(3)\n",
    "\n",
    "# 存储为 DataFrame\n",
    "df = pd.DataFrame(data_list, columns=[\"店铺名称\", \"链接\", \"图片链接\", \"评分\", \"评价数量\", \"人均消费\", \"地理位置\", \"推荐菜品\"])\n",
    "\n",
    "# 保存为 CSV 文件，确保中文显示\n",
    "df.to_csv(\"重庆美食餐厅.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"数据爬取完成，已保存为重庆美食餐厅.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd\n",
    "\n",
    "# 随机生成 User-Agent\n",
    "def get_random_useragent():\n",
    "    ua = UserAgent()\n",
    "    return ua.random\n",
    "\n",
    "# 用于更新 headers\n",
    "def update_headers(cookie):\n",
    "    headers = {\n",
    "        \"User-Agent\": get_random_useragent(),  # 使用随机UA避免被封\n",
    "        \"Cookie\": cookie\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "# 解析评论数据\n",
    "def parse_reviews(contents):\n",
    "    review_list = []\n",
    "\n",
    "    # 正则表达式提取所需信息\n",
    "    user_id_pattern = re.compile(r'<a class=\"name\".*?data-user-id=\"(?P<user_id>\\d+)\"', re.S)\n",
    "    overall_score_pattern = re.compile(r'<span class=\"sml-rank-stars sml-str(?P<score>\\d+).*?</span>', re.S)\n",
    "    taste_score_pattern = re.compile(r'口味：(?P<taste>\\d+\\.\\d+)', re.S)\n",
    "    environment_score_pattern = re.compile(r'环境：(?P<environment>\\d+\\.\\d+)', re.S)\n",
    "    service_score_pattern = re.compile(r'服务：(?P<service>\\d+\\.\\d+)', re.S)\n",
    "    comment_pattern = re.compile(r'<div class=\"review-words.*?>(?P<comment>.*?)</div>', re.S)\n",
    "    liked_dish_pattern = re.compile(r'喜欢的菜：.*?<a.*?>(?P<dish>.*?)</a>', re.S)\n",
    "    time_pattern = re.compile(r'<span class=\"time\">\\s*(?P<time>.*?)\\s*</span>', re.S)\n",
    "\n",
    "    user_ids = user_id_pattern.findall(contents)\n",
    "    overall_scores = overall_score_pattern.findall(contents)\n",
    "    taste_scores = taste_score_pattern.findall(contents)\n",
    "    environment_scores = environment_score_pattern.findall(contents)\n",
    "    service_scores = service_score_pattern.findall(contents)\n",
    "    comments = comment_pattern.findall(contents)\n",
    "    liked_dishes = liked_dish_pattern.findall(contents)\n",
    "    times = time_pattern.findall(contents)\n",
    "\n",
    "    # 处理评论内容\n",
    "    processed_comments = []\n",
    "    for comment in comments:\n",
    "        processed_comment = re.sub(r\"&#x0A;|&#x20;|<img.*?>\", \"\", comment).strip()\n",
    "        processed_comments.append(processed_comment)\n",
    "\n",
    "    # 补全缺失数据\n",
    "    max_len = max(len(user_ids), len(overall_scores), len(taste_scores), len(environment_scores),\n",
    "                  len(service_scores), len(processed_comments), len(times))\n",
    "\n",
    "    user_ids += [\"匿名\"] * (max_len - len(user_ids))\n",
    "    overall_scores += [\"0\"] * (max_len - len(overall_scores))\n",
    "    taste_scores += [\"0\"] * (max_len - len(taste_scores))\n",
    "    environment_scores += [\"0\"] * (max_len - len(environment_scores))\n",
    "    service_scores += [\"0\"] * (max_len - len(service_scores))\n",
    "    processed_comments += [\"无\"] * (max_len - len(processed_comments))\n",
    "    times += [\"未知时间\"] * (max_len - len(times))\n",
    "    liked_dishes += [\"无\"] * (max_len - len(liked_dishes))\n",
    "\n",
    "    for i in range(max_len):\n",
    "        review_list.append([\n",
    "            user_ids[i],\n",
    "            overall_scores[i],\n",
    "            taste_scores[i],\n",
    "            environment_scores[i],\n",
    "            service_scores[i],\n",
    "            processed_comments[i],\n",
    "            liked_dishes[i],\n",
    "            times[i]\n",
    "        ])\n",
    "\n",
    "    return review_list\n",
    "\n",
    "# 清洗评论内容的函数\n",
    "def clean_comment_v3(comment):\n",
    "    # Remove spaces first\n",
    "    comment = comment.replace(\" \", \"\").strip()\n",
    "    # Remove the entire <a> tag that contains '收起评价' and its content\n",
    "    comment = re.sub(r'<a[^>]*>收起评价.*?</a>', '', comment)\n",
    "    # Also remove any remaining tags that might have been missed\n",
    "    comment = re.sub(r'<[^>]+>', '', comment)\n",
    "    comment = comment.replace(\"收起评价\", \"\").strip()\n",
    "    return comment\n",
    "\n",
    "# 爬取评论\n",
    "def scrape_comments(shop_id, min_page, max_page, headers):\n",
    "    filename = f\"FOODBOWL超级碗(中信泰富广场店).csv\"\n",
    "\n",
    "    # 打开 CSV 文件\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow([\"用户ID\", \"总体评分\", \"口味评分\", \"环境评分\", \"服务评分\", \"评论内容\", \"喜欢的菜\", \"评论时间\"])\n",
    "\n",
    "        for i in tqdm(range(min_page, max_page + 1)):\n",
    "            url = f\"https://www.dianping.com/shop/{shop_id}/review_all/p{i}\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.encoding = \"utf-8-sig\"\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                reviews = parse_reviews(response.text)\n",
    "                for review in reviews:\n",
    "                    csv_writer.writerow(review)\n",
    "                print(f\"第 {i} 页爬取完成\")\n",
    "            else:\n",
    "                print(f\"第 {i} 页请求失败，状态码：{response.status_code}\")\n",
    "\n",
    "            # 随机休眠，避免被封\n",
    "            time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    print(f\"所有评论爬取完成，已保存至 {filename}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 用户输入信息\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'  # 您可以直接粘贴 UA\n",
    "    cookie = 'cy=1; cityid=1; cye=shanghai; GSI=#sl_zdjlwxbdsb@211011l#rf_tsmbyrk@220309b#rf_syztwlb@210720a#rf_syjgwlb@220829a; _lxsdk_cuid=193b544a72fc8-026d202573c394-26011851-144000-193b544a72fc8; _lxsdk=193b544a72fc8-026d202573c394-26011851-144000-193b544a72fc8; _hc.v=65c3183f-14b4-7d2c-f46d-88a102276353.1733912996; s_ViewType=10; ctu=5f9373f1a224b576a5e42830c308791e18489f3daf5c11028df005d64e30d897; cityid=98; fspop=test; Hm_lvt_602b80cf8079ae6591966cc70a3940e7=1738237847; HMACCOUNT=C2A9454786D9FDA0; __CACHE@is_login=true; logan_custom_report=; cityname=%E9%95%87%E6%B1%9F; cy=1; cye=shanghai; WEBDFPID=uuvwz3z02v21506wy331v8x002z4yzyu806y455wz479795861zw4zzu-1738586097610-1733913006752GAGAESCfd79fef3d01d5e9aadc18ccd4d0c95073349; _lx_utm=utm_source%3Dgoogle%26utm_medium%3Dorganic; Hm_lvt_6bc30b61e75e1c8b9c509994370bd6e9=1738499775; Hm_lpvt_6bc30b61e75e1c8b9c509994370bd6e9=1738499789; __CACHE@referer=https://www.dianping.com/search/keyword/1/0_%E8%B6%85%E7%BA%A7%E7%A2%97; logan_session_token=027lzmsxi29nqci16sp9; qruuid=f18980b0-858a-46be-8ac7-49d87bee42f2; dplet=191ae46df36173e0d774b3932768cb70; dper=0202541f74b9e701a749b2ce6a18a2dc7252ba55d26eec122087e5889347327e98436c5f19d8bee4ee05eb6866afc6ec3872e13425c20b450a0b0000000084260000197e371587874273c434977baaef554b85e5333ccf63221d470c8dc9b3417a75bc825b9f093cfd4dadf81623aa0d20b4; ll=7fd06e815b796be3df069dec7836c3df; ua=dpuser_3514064540; Hm_lpvt_602b80cf8079ae6591966cc70a3940e7=1738509485; _lxsdk_s=194c6a6a762-858-10d-972%7C%7C3957'  # 直接粘贴您的 Cookie\n",
    "    shop_id = 'l2yPNFK0pLypmHAQ'\n",
    "    min_page = int(input(\"请输入最小页码：\"))\n",
    "    max_page = int(input(\"请输入最大页码：\"))\n",
    "\n",
    "    headers = update_headers(cookie)\n",
    "    scrape_comments(shop_id, min_page, max_page, headers)\n",
    "    # 加载数据\n",
    "    data = pd.read_csv('FOODBOWL超级碗(中信泰富广场店).csv')\n",
    "\n",
    "    # 清洗评论内容\n",
    "    data['评论内容'] = data['评论内容'].apply(clean_comment_v3)\n",
    "\n",
    "    # 保存清洗后的数据到新的CSV文件\n",
    "    cleaned_file_path = '15.FOODBOWL超级碗(中信泰富广场店).csv'\n",
    "    data.to_csv(cleaned_file_path, index=False, encoding='utf-8-sig')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
