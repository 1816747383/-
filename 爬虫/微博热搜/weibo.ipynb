{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f600a893",
   "metadata": {},
   "source": [
    "### 热搜（request，爬不了详情页）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0f381f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "排 1 | 淘宝免单答案清一色                      | 热度 1296606 | 阅读 -        | 讨论 -      | 互动 -      | 原创 -\n",
      "排 2 | 全红婵家新房盖好可拎包入住                  | 热度 688267  | 阅读 -        | 讨论 -      | 互动 -      | 原创 -\n",
      "排 3 | 一图速览一揽子金融增量政策                  | 热度 579872  | 阅读 -        | 讨论 -      | 互动 -      | 原创 -\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 144\u001b[0m\n\u001b[0;32m    141\u001b[0m     save_to_csv(hot_list)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 144\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 128\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# 2. 遍历抓详情\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m hot_list:\n\u001b[1;32m--> 128\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     reading, discussion, interaction, original \u001b[38;5;241m=\u001b[39m fetch_detail_stats(item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail_url\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    130\u001b[0m     item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreading\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m reading\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "爬取微博热搜前50条：排名、标题、热度、阅读量、讨论量、详情页URL，\n",
    "并保存为 CSV（weibo_hot.csv）\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import urllib.parse\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# 全局请求头，模拟浏览器\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 \",\n",
    "    \"cookie\": \"SCF=AvSKhFc03l4Rh_j2xSX7gPrzpY_yODV2-61lZhom9tPuwknwCFkXdReRlqoKjMmde6M4GnVvZyHIOVBHBC8-wko.; SINAGLOBAL=4918828378775.037.1734511921189; UOR=,,www.google.com; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WWxuIxHTcvTlbN0RPpN9Wrv5JpX5KMhUgL.FoMEeh5pSoece052dJLoIp9jIg_Li--Ni-82iKn4i--4i-20iKy8; ALF=1749259237; SUB=_2A25FGHa1DeRhGeFM61IQ9i3KyDyIHXVmVPZ9rDV8PUJbkNAYLXXwkW1NQOJQREFBoWuCRYSs3KTFsZ2QH_IENf1m; _s_tentry=www.google.com; Apache=9360931081077.062.1746667312560; ULV=1746667312573:6:1:1:9360931081077.062.1746667312560:1742028222529\",\n",
    "}\n",
    "\n",
    "def get_hot_search_list(limit=50):\n",
    "    \"\"\" 获取微博热搜列表页前 N 条数据（遍历所有 tr，遇到符合格式的就解析） \"\"\"\n",
    "    url = \"https://s.weibo.com/top/summary?cate=realtimehot\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    resp.encoding = resp.apparent_encoding\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "    hot_list = []\n",
    "    for tr in soup.find_all(\"tr\"):\n",
    "        rank_td = tr.find(\"td\", class_=\"td-01\")\n",
    "        title_td = tr.find(\"td\", class_=\"td-02\")\n",
    "        if not rank_td or not title_td:\n",
    "            continue\n",
    "\n",
    "        rank_text = rank_td.get_text(strip=True)\n",
    "        if not rank_text.isdigit():\n",
    "            continue\n",
    "\n",
    "        a_tag = title_td.find(\"a\")\n",
    "        title = a_tag.get_text(strip=True) if a_tag else \"\"\n",
    "        heat_span = title_td.find(\"span\")\n",
    "        heat = heat_span.get_text(strip=True) if heat_span else \"\"\n",
    "\n",
    "        href = a_tag.get(\"href\", \"\") if a_tag else \"\"\n",
    "        full_link = urllib.parse.urljoin(\"https://s.weibo.com\", href)\n",
    "        params = urllib.parse.parse_qs(urllib.parse.urlparse(full_link).query)\n",
    "        q = params.get(\"q\", [None])[0]\n",
    "        if q:\n",
    "            q_enc = urllib.parse.quote(q, safe=\"\")\n",
    "            detail_url = f\"https://m.s.weibo.com/vtopic/detail_new?click_from=searchpc&q={q_enc}\"\n",
    "        else:\n",
    "            detail_url = None\n",
    "\n",
    "        hot_list.append({\n",
    "            \"rank\": int(rank_text),\n",
    "            \"title\": title,\n",
    "            \"heat\": heat,\n",
    "            \"detail_url\": detail_url,\n",
    "        })\n",
    "\n",
    "        if len(hot_list) >= limit:\n",
    "            break\n",
    "\n",
    "    if not hot_list:\n",
    "        print(\"⚠️ 没有抓到任何热搜，可能被反爬或页面结构再改动，请检查网络／Cookie／Headers。\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    return hot_list\n",
    "\n",
    "def fetch_detail_stats(detail_url):\n",
    "    \"\"\" 根据详情页 HTML，提取阅读量、讨论量、互动量、原创量 \"\"\"\n",
    "    if not detail_url:\n",
    "        return None, None, None, None\n",
    "    resp = requests.get(detail_url, headers=HEADERS, timeout=10)\n",
    "    resp.encoding = resp.apparent_encoding\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "    detail_data = soup.find(\"div\", class_=\"detail-data\")\n",
    "    if not detail_data:\n",
    "        return None, None, None, None\n",
    "\n",
    "    reading = discussion = interaction = original = None\n",
    "    for item in detail_data.find_all(\"div\", class_=\"item-col\"):\n",
    "        label = item.find(\"div\", class_=\"des\").get_text(strip=True)\n",
    "        num_div = item.find(\"div\", class_=\"num\")\n",
    "        num_text = \"\".join(num_div.stripped_strings)\n",
    "        if label == \"阅读量\":\n",
    "            reading = num_text\n",
    "        elif label == \"讨论量\":\n",
    "            discussion = num_text\n",
    "        elif label == \"互动量\":\n",
    "            interaction = num_text\n",
    "        elif label == \"原创量\":\n",
    "            original = num_text\n",
    "\n",
    "    return reading, discussion, interaction, original\n",
    "\n",
    "def save_to_csv(data_list, filename=\"weibo_hot.csv\"):\n",
    "    \"\"\" 将结果列表保存为 CSV 文件 \"\"\"\n",
    "    fieldnames = [\n",
    "        \"rank\", \"title\", \"heat\",\n",
    "        \"reading\", \"discussion\", \"interaction\", \"original\",\n",
    "        \"detail_url\"\n",
    "    ]\n",
    "    with open(filename, mode=\"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for item in data_list:\n",
    "            writer.writerow({\n",
    "                \"rank\": item.get(\"rank\", \"\"),\n",
    "                \"title\": item.get(\"title\", \"\"),\n",
    "                \"heat\": item.get(\"heat\", \"\"),\n",
    "                \"reading\": item.get(\"reading\", \"\"),\n",
    "                \"discussion\": item.get(\"discussion\", \"\"),\n",
    "                \"interaction\": item.get(\"interaction\", \"\"),\n",
    "                \"original\": item.get(\"original\", \"\"),\n",
    "                \"detail_url\": item.get(\"detail_url\", \"\"),\n",
    "            })\n",
    "    print(f\"✅ 已保存到文件：{filename}\")\n",
    "\n",
    "def main():\n",
    "    # 1. 抓列表\n",
    "    hot_list = get_hot_search_list(50)\n",
    "\n",
    "    # 2. 遍历抓详情\n",
    "    for item in hot_list:\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        reading, discussion, interaction, original = fetch_detail_stats(item[\"detail_url\"])\n",
    "        item[\"reading\"] = reading\n",
    "        item[\"discussion\"] = discussion\n",
    "        item[\"interaction\"] = interaction\n",
    "        item[\"original\"] = original\n",
    "        print(\n",
    "            f\"排{item['rank']:>2} | {item['title']:<30} | 热度 {item['heat']:<7} | \"\n",
    "            f\"阅读 {reading or '-':<8} | 讨论 {discussion or '-':<6} | \"\n",
    "            f\"互动 {interaction or '-':<6} | 原创 {original or '-'}\"\n",
    "        )\n",
    "\n",
    "    # 3. 保存 CSV\n",
    "    save_to_csv(hot_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b33985",
   "metadata": {},
   "source": [
    "### 热搜榜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef27e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "排 1 | 淘宝免单答案清一色            | 热度 1306413 | 阅671.5万   讨4922   互7114   原3358\n",
      "排 2 | 全红婵家新房盖好可拎包入住        | 热度 681177  | 阅1087.2万  讨486    互3176   原114\n",
      "排 3 | 一图速览一揽子金融增量政策        | 热度 662678  | 阅2582万    讨2121   互4148   原467\n",
      "排 4 | 泡泡玛特股份被创始股东高位清仓      | 热度 653847  | 阅3373.7万  讨3208   互1.4万   原876\n",
      "排 5 | 中央气象台连发3预警           | 热度 590091  | 阅1561.8万  讨3625   互7256   原779\n",
      "排 6 | 团队曝异瞳少年治沙为摆拍         | 热度 577478  | 阅4035.3万  讨3481   互2.6万   原1117\n",
      "排 7 | 特朗普将宣布波斯湾改称阿拉伯湾      | 热度 512196  | 阅2249.5万  讨2074   互1.3万   原733\n",
      "排 8 | 小女孩拿自家金饰去卖老板直接扣下     | 热度 390157  | 阅1674.6万  讨2461   互1.1万   原458\n",
      "排 9 | 学生班级搞孤立老师坚定停课惩治      | 热度 356428  | 阅6665.8万  讨1.3万   互7万     原2533\n",
      "排10 | 被刘嘉玲认证超级帅哥的含金量       | 热度 354191  | 阅2365.1万  讨2.1万   互7.5万   原6121\n",
      "排11 | 杜淳妻子发长文给未婚女生十点建议     | 热度 346319  | 阅2940.7万  讨2245   互1.1万   原547\n",
      "排12 | 印巴125架战机激战1小时        | 热度 342338  | 阅866.6万   讨1025   互7894   原281\n",
      "排13 | 悟空                   | 热度 336351  | 阅2.6亿     讨17.1万  互30.7万  原2.3万\n",
      "排14 | 美联储主席发布会提及关税超20次     | 热度 334026  | 阅95.5万    讨112    互227    原28\n",
      "排15 | 奈雪的茶再换logo改成奈雪       | 热度 328547  | 阅4453.4万  讨3243   互1.4万   原820\n",
      "排16 | 印巴局势再升级              | 热度 325627  | 阅5435.8万  讨1.4万   互7.3万   原1506\n",
      "排17 | 谍战上不封顶 周冬雨           | 热度 324306  | 阅93.8万    讨694    互2301   原226\n",
      "排18 | 丁真说演十八铜人把自己害惨了       | 热度 综艺 320173 | 阅972.8万   讨4304   互1.4万   原327\n",
      "排19 | Bin霸气发言              | 热度 314847  | 阅356.5万   讨885    互4879   原84\n",
      "排20 | 国乒结束成都封训             | 热度 311948  | 阅80.3万    讨51     互244    原22\n",
      "排21 | 女子吐槽半个玉米扣蛋挞上50元      | 热度 309658  | 阅1480.1万  讨1703   互9520   原350\n",
      "排22 | 男子辞职照顾2岁妈妈回应啃老质疑     | 热度 304189  | 阅529.1万   讨418    互3211   原108\n",
      "排23 | 巴西女硕士夸日本安全次日疑遭焚尸     | 热度 296428  | 阅3202.3万  讨1717   互9965   原281\n",
      "排24 | 李冰冰于适手挽手             | 热度 272874  | 阅5591.2万  讨1.6万   互8.3万   原6585\n",
      "排25 | 体检报告出现这些词要当心         | 热度 267775  | 阅2120.8万  讨1504   互8848   原994\n",
      "排26 | 特朗普号召美国民众勒紧裤腰带       | 热度 265095  | 阅1174.9万  讨2116   互1.3万   原612\n",
      "排27 | 阳光灿烂的笑容有多治愈          | 热度 253591  | 阅323.2万   讨4722   互1.1万   原382\n",
      "排28 | 李宏毅杨雨潼同框好养眼          | 热度 232288  | 阅141.1万   讨6060   互1.7万   原506\n",
      "排29 | 张艺兴曾发行名为酒的歌曲         | 热度 音乐 225987 | 阅823.1万   讨1465   互1.9万   原314\n",
      "排30 | 布云朝克特0比2达尔德里         | 热度 222931  | 阅58.9万    讨61     互150    原28\n",
      "排31 | 白鹿回老家了               | 热度 220815  | 阅5938.5万  讨2.8万   互6.5万   原1.3万\n",
      "排32 | 女子上门做饭每天6单月薪近2万      | 热度 218313  | 阅1078万    讨1904   互7093   原285\n",
      "排33 | 外国神秘大佬来北京吃火锅         | 热度 218195  | 阅298.3万   讨1573   互3175   原322\n",
      "排34 | 北京公积金贷款利率下调          | 热度 212709  | 阅38.7万    讨79     互96     原47\n",
      "排35 | 小米汽车                 | 热度 208412  | 阅13.1亿    讨125.7万 互405.6万 原10.2万\n",
      "排36 | 小米SU7 Ultra量产版       | 热度 186732  | 阅14.5万    讨109    互271    原40\n",
      "排37 | 直播间卖土鸡结果土是商标名        | 热度 150342  | 阅4661.1万  讨5243   互2.2万   原1161\n",
      "排38 | 奈雪内部人士回应改名           | 热度 146838  | 阅200万     讨36     互99     原21\n",
      "排39 | 印巴背后或涉及中美博弈          | 热度 138796  | 阅99.3万    讨387    互1364   原102\n",
      "排40 | 易烊千玺网站更新             | 热度 130484  | 阅4.5亿     讨50.9万  互128.9万 原5万\n",
      "排41 | 乘务员错把酒当水倒给3岁小孩       | 热度 129488  | 阅234.1万   讨71     互209    原20\n",
      "排42 | 中国留学生在马来西亚遭绑架后获救     | 热度 120945  | 阅359.7万   讨183    互1207   原37\n",
      "排43 | 鲜枣的维C含量是苹果的近百倍       | 热度 120379  | 阅142万     讨416    互695    原100\n",
      "排44 | 美联储宣布维持基准利率不变        | 热度 115155  | 阅2005.7万  讨4920   互1.4万   原726\n",
      "排45 | 清一色                  | 热度 112971  | 阅3718.2万  讨3.7万   互5.5万   原1.8万\n",
      "排46 | 泡泡玛特大跌超5%            | 热度 111368  | 阅130.5万   讨50     互127    原41\n",
      "排47 | 江苏将有8级以上阵风           | 热度 105883  | 阅316.8万   讨218    互424    原83\n",
      "排48 | 全款140万捡漏230平房子       | 热度 104164  | 阅1307万    讨1358   互5353   原309\n",
      "排49 | 俄罗斯政府高级官员热情迎接        | 热度 101979  | 阅561.7万   讨347    互2435   原60\n",
      "排50 | 小猫也会有自己最偏爱的人         | 热度 99663   | 阅213.6万   讨791    互1512   原79\n",
      "✅ 数据已保存到 weibo_hot.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "爬取微博热搜前50条：排名、标题、热度，\n",
    "用 Selenium（手机 UA+无界面）渲染详情页，提取阅读量、讨论量、互动量、原创量，\n",
    "并保存为 CSV（weibo_hot.csv）\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# —— 全局配置 —— #\n",
    "\n",
    "# PC 端列表页用的请求头\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 \",\n",
    "    \"cookie\": \"SCF=AvSKhFc03l4Rh_j2xSX7gPrzpY_yODV2-61lZhom9tPuwknwCFkXdReRlqoKjMmde6M4GnVvZyHIOVBHBC8-wko.; SINAGLOBAL=4918828378775.037.1734511921189; UOR=,,www.google.com; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WWxuIxHTcvTlbN0RPpN9Wrv5JpX5KMhUgL.FoMEeh5pSoece052dJLoIp9jIg_Li--Ni-82iKn4i--4i-20iKy8; ALF=1749259237; SUB=_2A25FGHa1DeRhGeFM61IQ9i3KyDyIHXVmVPZ9rDV8PUJbkNAYLXXwkW1NQOJQREFBoWuCRYSs3KTFsZ2QH_IENf1m; _s_tentry=www.google.com; Apache=9360931081077.062.1746667312560; ULV=1746667312573:6:1:1:9360931081077.062.1746667312560:1742028222529\",\n",
    "}\n",
    "\n",
    "# 移动端 \n",
    "MOBILE_UA = (\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) \"\n",
    "    \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 \"\n",
    "    \"Mobile/15E148 Safari/604.1\"\n",
    ")\n",
    "\n",
    "# Selenium 无界面 \n",
    "chrome_opts = Options()\n",
    "chrome_opts.add_argument(\"--headless\")\n",
    "chrome_opts.add_argument(\"--disable-gpu\")\n",
    "chrome_opts.add_argument(\"--no-sandbox\")\n",
    "chrome_opts.add_argument(f\"--user-agent={MOBILE_UA}\")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_opts)\n",
    "\n",
    "\n",
    "def get_hot_search_list(limit=50):\n",
    "    \"\"\" 用 requests 抓 PC 端热搜列表，返回前 N 条的 rank/title/heat/detail_url \"\"\"\n",
    "    url = \"https://s.weibo.com/top/summary?cate=realtimehot\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    resp.encoding = resp.apparent_encoding\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "    hot = []\n",
    "    for tr in soup.find_all(\"tr\"):\n",
    "        td1 = tr.find(\"td\", class_=\"td-01\")\n",
    "        td2 = tr.find(\"td\", class_=\"td-02\")\n",
    "        if not td1 or not td2:\n",
    "            continue\n",
    "        rank = td1.get_text(strip=True)\n",
    "        if not rank.isdigit():\n",
    "            continue\n",
    "\n",
    "        a = td2.find(\"a\")\n",
    "        title = a.get_text(strip=True) if a else \"\"\n",
    "        heat = td2.find(\"span\").get_text(strip=True) if td2.find(\"span\") else \"\"\n",
    "\n",
    "        href = a.get(\"href\", \"\") if a else \"\"\n",
    "        # PC 列表里 href 是 /weibo?q=...，直接跳到移动端渲染页面\n",
    "        params = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)\n",
    "        q = params.get(\"q\", [None])[0]\n",
    "        if q:\n",
    "            q_enc = urllib.parse.quote(q, safe=\"\")\n",
    "            detail_url = f\"https://m.s.weibo.com/vtopic/detail_new?click_from=searchpc&q={q_enc}\"\n",
    "        else:\n",
    "            detail_url = None\n",
    "\n",
    "        hot.append({\n",
    "            \"rank\": int(rank),\n",
    "            \"title\": title,\n",
    "            \"heat\": heat,\n",
    "            \"detail_url\": detail_url\n",
    "        })\n",
    "        if len(hot) >= limit:\n",
    "            break\n",
    "\n",
    "    if not hot:\n",
    "        print(\"⚠️ 热搜列表为空，请检查网络或被反爬拦截。\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    return hot\n",
    "\n",
    "\n",
    "def fetch_detail_stats(detail_url):\n",
    "    \"\"\"\n",
    "    用 Selenium（移动端 UA）渲染 detail_url，\n",
    "    提取：阅读量、讨论量、互动量、原创量\n",
    "    \"\"\"\n",
    "    driver.get(detail_url)\n",
    "    time.sleep(2)  # 等待 JS 渲染\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    box = soup.find(\"div\", class_=\"detail-data\")\n",
    "    if not box:\n",
    "        return None, None, None, None\n",
    "\n",
    "    reading = discussion = interaction = original = None\n",
    "    for item in box.find_all(\"div\", class_=\"item-col\"):\n",
    "        label = item.find(\"div\", class_=\"des\").get_text(strip=True)\n",
    "        num = \"\".join(item.find(\"div\", class_=\"num\").stripped_strings)\n",
    "        if label == \"阅读量\":\n",
    "            reading = num\n",
    "        elif label == \"讨论量\":\n",
    "            discussion = num\n",
    "        elif label == \"互动量\":\n",
    "            interaction = num\n",
    "        elif label == \"原创量\":\n",
    "            original = num\n",
    "\n",
    "    return reading, discussion, interaction, original\n",
    "\n",
    "\n",
    "def save_to_csv(data, filename=\"weibo_hot.csv\"):\n",
    "    \"\"\" 保存到 CSV \"\"\"\n",
    "    fields = [\n",
    "        \"rank\", \"title\", \"heat\",\n",
    "        \"reading\", \"discussion\", \"interaction\", \"original\",\n",
    "        \"detail_url\"\n",
    "    ]\n",
    "    with open(filename, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "    print(f\"✅ 数据已保存到 {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    hot_list = get_hot_search_list(50)\n",
    "    for item in hot_list:\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        r, d, i, o = fetch_detail_stats(item[\"detail_url\"])\n",
    "        item[\"reading\"], item[\"discussion\"], item[\"interaction\"], item[\"original\"] = r, d, i, o\n",
    "        print(f\"排{item['rank']:>2} | {item['title']:<20} | 热度 {item['heat']:<7} | \"\n",
    "              f\"阅{r or '-':<8} 讨{d or '-':<6} 互{i or '-':<6} 原{o or '-'}\")\n",
    "\n",
    "    save_to_csv(hot_list)\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c6e57c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "排 1 | 杜淳妻子发长文给未婚女生十点建议     | 热度 1185513 | 阅4122.8万  讨3074   互1.6万   原822\n",
      "排 2 | 悟空                   | 热度 676709  | 阅2.6亿     讨17.3万  互32万    原2.5万\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 150\u001b[0m\n\u001b[0;32m    146\u001b[0m     driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 139\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    137\u001b[0m hot_list \u001b[38;5;241m=\u001b[39m get_hot_search_list(\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m hot_list:\n\u001b[1;32m--> 139\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     r, d, i, o \u001b[38;5;241m=\u001b[39m fetch_detail_stats(item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail_url\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    141\u001b[0m     item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreading\u001b[39m\u001b[38;5;124m\"\u001b[39m], item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscussion\u001b[39m\u001b[38;5;124m\"\u001b[39m], item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteraction\u001b[39m\u001b[38;5;124m\"\u001b[39m], item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m r, d, i, o\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "爬取微博热搜前50条：排名、标题、热度，\n",
    "用 Selenium（手机 UA+无界面）渲染详情页，提取阅读量、讨论量、互动量、原创量，\n",
    "并保存为 CSV（weibo_hot.csv）\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# —— 全局配置 —— #\n",
    "\n",
    "# PC 端列表页用的请求头\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 \",\n",
    "    \"cookie\": \"SCF=AvSKhFc03l4Rh_j2xSX7gPrzpY_yODV2-61lZhom9tPuwknwCFkXdReRlqoKjMmde6M4GnVvZyHIOVBHBC8-wko.; SINAGLOBAL=4918828378775.037.1734511921189; UOR=,,www.google.com; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WWxuIxHTcvTlbN0RPpN9Wrv5JpX5KMhUgL.FoMEeh5pSoece052dJLoIp9jIg_Li--Ni-82iKn4i--4i-20iKy8; ALF=1749259237; SUB=_2A25FGHa1DeRhGeFM61IQ9i3KyDyIHXVmVPZ9rDV8PUJbkNAYLXXwkW1NQOJQREFBoWuCRYSs3KTFsZ2QH_IENf1m; _s_tentry=www.google.com; Apache=9360931081077.062.1746667312560; ULV=1746667312573:6:1:1:9360931081077.062.1746667312560:1742028222529\",\n",
    "}\n",
    "\n",
    "# 移动端 \n",
    "MOBILE_UA = (\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) \"\n",
    "    \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 \"\n",
    "    \"Mobile/15E148 Safari/604.1\"\n",
    ")\n",
    "\n",
    "# Selenium 无界面 \n",
    "chrome_opts = Options()\n",
    "chrome_opts.add_argument(\"--headless\")\n",
    "chrome_opts.add_argument(\"--disable-gpu\")\n",
    "chrome_opts.add_argument(\"--no-sandbox\")\n",
    "chrome_opts.add_argument(f\"--user-agent={MOBILE_UA}\")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_opts)\n",
    "\n",
    "\n",
    "def get_hot_search_list(limit=50):\n",
    "    \"\"\" 用 requests 抓 PC 端热搜列表，返回前 N 条的 rank/title/heat/detail_url \"\"\"\n",
    "    url = \"https://s.weibo.com/top/summary?cate=entrank\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    resp.encoding = resp.apparent_encoding\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "    hot = []\n",
    "    for tr in soup.find_all(\"tr\"):\n",
    "        td1 = tr.find(\"td\", class_=\"td-01\")\n",
    "        td2 = tr.find(\"td\", class_=\"td-02\")\n",
    "        if not td1 or not td2:\n",
    "            continue\n",
    "        rank = td1.get_text(strip=True)\n",
    "        if not rank.isdigit():\n",
    "            continue\n",
    "\n",
    "        a = td2.find(\"a\")\n",
    "        title = a.get_text(strip=True) if a else \"\"\n",
    "        heat = td2.find(\"span\").get_text(strip=True) if td2.find(\"span\") else \"\"\n",
    "\n",
    "        href = a.get(\"href\", \"\") if a else \"\"\n",
    "        # PC 列表里 href 是 /weibo?q=...，直接跳到移动端渲染页面\n",
    "        params = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)\n",
    "        q = params.get(\"q\", [None])[0]\n",
    "        if q:\n",
    "            q_enc = urllib.parse.quote(q, safe=\"\")\n",
    "            detail_url = f\"https://m.s.weibo.com/vtopic/detail_new?click_from=searchpc&q={q_enc}\"\n",
    "        else:\n",
    "            detail_url = None\n",
    "\n",
    "        hot.append({\n",
    "            \"rank\": int(rank),\n",
    "            \"title\": title,\n",
    "            \"heat\": heat,\n",
    "            \"detail_url\": detail_url\n",
    "        })\n",
    "        if len(hot) >= limit:\n",
    "            break\n",
    "\n",
    "    if not hot:\n",
    "        print(\"⚠️ 热搜列表为空，请检查网络或被反爬拦截。\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    return hot\n",
    "\n",
    "\n",
    "def fetch_detail_stats(detail_url):\n",
    "    \"\"\"\n",
    "    用 Selenium（移动端 UA）渲染 detail_url，\n",
    "    提取：阅读量、讨论量、互动量、原创量\n",
    "    \"\"\"\n",
    "    driver.get(detail_url)\n",
    "    time.sleep(2)  # 等待 JS 渲染\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    box = soup.find(\"div\", class_=\"detail-data\")\n",
    "    if not box:\n",
    "        return None, None, None, None\n",
    "\n",
    "    reading = discussion = interaction = original = None\n",
    "    for item in box.find_all(\"div\", class_=\"item-col\"):\n",
    "        label = item.find(\"div\", class_=\"des\").get_text(strip=True)\n",
    "        num = \"\".join(item.find(\"div\", class_=\"num\").stripped_strings)\n",
    "        if label == \"阅读量\":\n",
    "            reading = num\n",
    "        elif label == \"讨论量\":\n",
    "            discussion = num\n",
    "        elif label == \"互动量\":\n",
    "            interaction = num\n",
    "        elif label == \"原创量\":\n",
    "            original = num\n",
    "\n",
    "    return reading, discussion, interaction, original\n",
    "\n",
    "\n",
    "def save_to_csv(data, filename=\"weibo_entrank.csv\"):\n",
    "    \"\"\" 保存到 CSV \"\"\"\n",
    "    fields = [\n",
    "        \"rank\", \"title\", \"heat\",\n",
    "        \"reading\", \"discussion\", \"interaction\", \"original\",\n",
    "        \"detail_url\"\n",
    "    ]\n",
    "    with open(filename, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "    print(f\"✅ 数据已保存到 {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    hot_list = get_hot_search_list(50)\n",
    "    for item in hot_list:\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        r, d, i, o = fetch_detail_stats(item[\"detail_url\"])\n",
    "        item[\"reading\"], item[\"discussion\"], item[\"interaction\"], item[\"original\"] = r, d, i, o\n",
    "        print(f\"排{item['rank']:>2} | {item['title']:<20} | 热度 {item['heat']:<7} | \"\n",
    "              f\"阅{r or '-':<8} 讨{d or '-':<6} 互{i or '-':<6} 原{o or '-'}\")\n",
    "\n",
    "    save_to_csv(hot_list)\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa3d92ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#一图速览一揽子金融增量政策#                | 阅读:2915.2万 讨论:2412 互动:4816 原创:529\n",
      "#泡泡玛特股份被创始股东高位清仓#              | 阅读:4186.7万 讨论:4175 互动:1.8万 原创:1045\n",
      "#中央气象台连发3预警#                   | 阅读:2263.2万 讨论:5737 互动:1.2万 原创:1234\n",
      "#乘务员错把酒当水倒给3岁小孩#               | 阅读:698.3万 讨论:305 互动:1674 原创:86\n",
      "#今天世界微笑日#                      | 阅读:1.2亿 讨论:7.9万 互动:18.2万 原创:5456\n",
      "#阳光灿烂的笑容有多治愈#                  | 阅读:541.4万 讨论:6692 互动:1.7万 原创:573\n",
      "#小女孩拿自家金饰去卖老板直接扣下#             | 阅读:2223.4万 讨论:3141 互动:1.5万 原创:608\n",
      "#学生班级搞孤立老师坚定停课惩治#              | 阅读:7197.5万 讨论:1.5万 互动:7.6万 原创:2662\n",
      "#江苏将有8级以上阵风#                   | 阅读:503.2万 讨论:468 互动:960 原创:161\n",
      "#特朗普号召美国民众勒紧裤腰带#               | 阅读:1526.9万 讨论:2956 互动:1.8万 原创:713\n",
      "#施工方回应全红婵老家盖别墅#                | 阅读:278.8万 讨论:81 互动:697 原创:25\n",
      "#美联储主席发布会提及关税超20次#             | 阅读:427.3万 讨论:351 互动:1235 原创:89\n",
      "#北京公积金贷款利率下调#                  | 阅读:481.1万 讨论:279 互动:483 原创:132\n",
      "#奈雪内部人士回应改名#                   | 阅读:357.3万 讨论:118 互动:390 原创:30\n",
      "#中国留学生在马来西亚遭绑架后获救#             | 阅读:584.8万 讨论:310 互动:2069 原创:72\n",
      "#美联储宣布维持基准利率不变#                | 阅读:2107.2万 讨论:5001 互动:1.4万 原创:738\n",
      "#美联储将继续减持美国国债#                 | 阅读:113.6万 讨论:49 互动:104 原创:26\n",
      "#多部门介绍民营经济促进法#                 | 阅读:169.8万 讨论:61 互动:142 原创:16\n",
      "#误给小孩葡萄酒航司不能只道歉赔偿#             | 阅读:249.1万 讨论:28 互动:68 原创:19\n",
      "#北约轰炸中牺牲的3位烈士雕像塑成#             | 阅读:128.1万 讨论:61 互动:318 原创:20\n",
      "#女孩cos石矶娘娘成景区打卡点#              | 阅读:75.6万 讨论:25 互动:82 原创:17\n",
      "#俄罗斯政府高级官员热情迎接#                | 阅读:615.6万 讨论:400 互动:2817 原创:67\n",
      "#直播间卖土鸡结果土是商标名#                | 阅读:4782万 讨论:5658 互动:2.3万 原创:1193\n",
      "#中国和俄罗斯是好邻居真朋友好伙伴#             | 阅读:286.6万 讨论:356 互动:1736 原创:42\n",
      "#巴基斯坦将对印军入侵做出回应#               | 阅读:1265.5万 讨论:1050 互动:5179 原创:137\n",
      "#美联储称美关税影响比预期大得多#              | 阅读:452万 讨论:2126 互动:4861 原创:237\n",
      "#美财长阿巴阿巴阿巴#                    | 阅读:365.4万 讨论:371 互动:2009 原创:29\n",
      "#胖东来起诉柴怼怼商业诋毁侵犯名誉权#            | 阅读:686.6万 讨论:792 互动:4196 原创:181\n",
      "#吴谢宇案人性的深渊#                    | 阅读:862.5万 讨论:1442 互动:5247 原创:50\n",
      "#拜登卸任后首次接受采访#                  | 阅读:1179.2万 讨论:656 互动:2126 原创:163\n",
      "#中方呼吁共同捍卫二战胜利成果#               | 阅读:396.5万 讨论:283 互动:949 原创:59\n",
      "#中俄关系不断焕发新活力#                  | 阅读:231.8万 讨论:319 互动:1510 原创:42\n",
      "#要求刷卡老人站着公交司机被停职#              | 阅读:1126.6万 讨论:1260 互动:7092 原创:193\n",
      "#全美最大港口货物量暴跌#                  | 阅读:221.4万 讨论:139 互动:346 原创:37\n",
      "#42岁退役军人为救人牺牲#                 | 阅读:428.6万 讨论:2069 互动:8643 原创:162\n",
      "#全美最大港口繁忙不再#                   | 阅读:363.8万 讨论:162 互动:772 原创:24\n",
      "#中俄元首继续引领两国关系登高望远#             | 阅读:741.6万 讨论:364 互动:2083 原创:88\n",
      "#搜救队排查岩壁溪谷寻找失联男童#              | 阅读:158.4万 讨论:33 互动:93 原创:17\n",
      "#债券市场科技板准备工作已基本就绪#             | 阅读:193万 讨论:63 互动:219 原创:16\n",
      "#在册在世慰安妇制度受害者仅剩7人#             | 阅读:190.6万 讨论:242 互动:668 原创:33\n",
      "#武大副教授被举报抄袭学生论文#               | 阅读:323.9万 讨论:305 互动:1775 原创:34\n",
      "#中国外贸企业说要做的是挺直脊梁#              | 阅读:200.7万 讨论:423 互动:1211 原创:61\n",
      "#中方呼吁印巴双方保持冷静克制#               | 阅读:319.5万 讨论:322 互动:957 原创:41\n",
      "#对原产印度进口氯氰菊酯实施反倾销#             | 阅读:561.9万 讨论:621 互动:2770 原创:80\n",
      "#输头孢过敏离世的女生家属发声#               | 阅读:551.8万 讨论:243 互动:1151 原创:30\n",
      "#保水虾仁4家涉事企业被罚超800万#            | 阅读:73.6万 讨论:38 互动:109 原创:16\n",
      "#工作人员回应深圳机场发生大面积延误#            | 阅读:191.9万 讨论:466 互动:5346 原创:120\n",
      "#美加领导人会晤#                      | 阅读:96.1万 讨论:67 互动:334 原创:20\n",
      "#妇科论文现男性患者涉事作者被处理#             | 阅读:166.9万 讨论:158 互动:2578 原创:38\n",
      "#中德两国理当携手抵御疾风骤雨#               | 阅读:460.8万 讨论:513 互动:2095 原创:43\n",
      "✅ 已保存到文件：weibo_socialevent.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "爬取微博热搜前50条：标题、详情页 URL（移动端），\n",
    "详情页使用 Selenium 渲染并提取阅读量、讨论量、互动量、原创量，\n",
    "并保存为 CSV（weibo_socialevent.csv）\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import urllib.parse\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "\n",
    "# —— 全局配置 —— #\n",
    "\n",
    "# 请求头，仅用于列表页\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\",\n",
    "    \"Cookie\": (\n",
    "        \"SCF=AvSKhFc03l4Rh_j2xSX7gPrzpY_yODV2-61lZhom9tPuwknwCFkXdReRlqoKjMmde6M4GnVvZyHIOVBHBC8-wko.; SINAGLOBAL=4918828378775.037.1734511921189; UOR=,,www.google.com; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WWxuIxHTcvTlbN0RPpN9Wrv5JpX5KMhUgL.FoMEeh5pSoece052dJLoIp9jIg_Li--Ni-82iKn4i--4i-20iKy8; ALF=1749259237; SUB=_2A25FGHa1DeRhGeFM61IQ9i3KyDyIHXVmVPZ9rDV8PUJbkNAYLXXwkW1NQOJQREFBoWuCRYSs3KTFsZ2QH_IENf1m; _s_tentry=www.google.com; Apache=9360931081077.062.1746667312560; ULV=1746667312573:6:1:1:9360931081077.062.1746667312560:1742028222529\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Selenium 配置：无界面 + 移动端 UA\n",
    "MOBILE_UA = (\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) \"\n",
    "    \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 \"\n",
    "    \"Mobile/15E148 Safari/604.1\"\n",
    ")\n",
    "chrome_opts = Options()\n",
    "chrome_opts.add_argument(\"--headless\")\n",
    "chrome_opts.add_argument(\"--disable-gpu\")\n",
    "chrome_opts.add_argument(\"--no-sandbox\")\n",
    "chrome_opts.add_argument(f\"--user-agent={MOBILE_UA}\")\n",
    "# 启动 WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_opts)\n",
    "\n",
    "def get_hot_search_list(limit=50):\n",
    "    \"\"\"获取热搜列表，返回标题与移动端详情 URL\"\"\"\n",
    "    url = \"https://s.weibo.com/top/summary?cate=socialevent\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    resp.encoding = resp.apparent_encoding\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "    hot_list = []\n",
    "    for tr in soup.find_all(\"tr\"):\n",
    "        a_tag = tr.select_one(\"td.td-02 > a\")\n",
    "        if not a_tag or not a_tag.has_attr(\"href\"):\n",
    "            continue\n",
    "        title = a_tag.get_text(strip=True)\n",
    "        href = a_tag[\"href\"]\n",
    "        # 构造移动端详情页\n",
    "        params = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)\n",
    "        q = params.get(\"q\", [None])[0]\n",
    "        if not q:\n",
    "            continue\n",
    "        q_enc = urllib.parse.quote(q, safe=\"\")\n",
    "        detail_url = f\"https://m.s.weibo.com/vtopic/detail_new?click_from=searchpc&q={q_enc}\"\n",
    "        hot_list.append({\"title\": title, \"detail_url\": detail_url})\n",
    "        if len(hot_list) >= limit:\n",
    "            break\n",
    "\n",
    "    if not hot_list:\n",
    "        print(\"⚠️ 未获取到热搜列表，请检查网络或 headers。\")\n",
    "        sys.exit(1)\n",
    "    return hot_list\n",
    "\n",
    "\n",
    "def fetch_detail_stats(detail_url):\n",
    "    \"\"\"使用 Selenium 渲染详情页，并提取阅读量、讨论量、互动量、原创量\"\"\"\n",
    "    driver.get(detail_url)\n",
    "    # 等待页面 JS 渲染\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    # 遍历 .item-col\n",
    "    reading = discussion = interaction = original = None\n",
    "    for item in soup.find_all(\"div\", class_=\"item-col\"):\n",
    "        label_tag = item.find(\"div\", class_=\"des\")\n",
    "        num_tag = item.find(\"div\", class_=\"num\")\n",
    "        if not label_tag or not num_tag:\n",
    "            continue\n",
    "        label = label_tag.get_text(strip=True)\n",
    "        num = \"\".join(num_tag.stripped_strings)\n",
    "        if label == \"阅读量\":\n",
    "            reading = num\n",
    "        elif label == \"讨论量\":\n",
    "            discussion = num\n",
    "        elif label == \"互动量\":\n",
    "            interaction = num\n",
    "        elif label == \"原创量\":\n",
    "            original = num\n",
    "    return reading, discussion, interaction, original\n",
    "\n",
    "\n",
    "def save_to_csv(data_list, filename=\"weibo_socialevent.csv\"):\n",
    "    \"\"\"保存数据到 CSV 文件\"\"\"\n",
    "    fieldnames = [\n",
    "        \"title\", \"detail_url\",\n",
    "        \"reading\", \"discussion\", \"interaction\", \"original\"\n",
    "    ]\n",
    "    with open(filename, mode=\"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for item in data_list:\n",
    "            writer.writerow({\n",
    "                \"title\": item.get(\"title\", \"\"),\n",
    "                \"detail_url\": item.get(\"detail_url\", \"\"),\n",
    "                \"reading\": item.get(\"reading\", \"\"),\n",
    "                \"discussion\": item.get(\"discussion\", \"\"),\n",
    "                \"interaction\": item.get(\"interaction\", \"\"),\n",
    "                \"original\": item.get(\"original\", \"\"),\n",
    "            })\n",
    "    print(f\"✅ 已保存到文件：{filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1. 获取热搜列表\n",
    "    hot_list = get_hot_search_list(50)\n",
    "    # 2. 遍历详情页\n",
    "    for item in hot_list:\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        r, d, i, o = fetch_detail_stats(item[\"detail_url\"])\n",
    "        item.update({\"reading\": r, \"discussion\": d, \"interaction\": i, \"original\": o})\n",
    "        print(f\"{item['title']:<30} | 阅读:{r or '-'} 讨论:{d or '-'} 互动:{i or '-'} 原创:{o or '-'}\")\n",
    "    # 3. 保存 CSV\n",
    "    save_to_csv(hot_list)\n",
    "    # 4. 退出浏览器\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba1e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b8e46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://s.weibo.com/top/summary?cate=socialevent\"\n",
    "resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "resp.encoding = resp.apparent_encoding\n",
    "soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "hot_list = []\n",
    "for tr in soup.find_all(\"tr\"):\n",
    "\n",
    "    a_tag = tr.select_one(\"td.td-02 > a\")\n",
    "    title = a_tag.get_text(strip=True) if a_tag else \"\"\n",
    "\n",
    "    href = a_tag[\"href\"] if a_tag else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56b8a0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/weibo?t=60&q=%23%E4%B8%AD%E5%BE%B7%E4%B8%A4%E5%9B%BD%E7%90%86%E5%BD%93%E6%90%BA%E6%89%8B%E6%8A%B5%E5%BE%A1%E7%96%BE%E9%A3%8E%E9%AA%A4%E9%9B%A8%23'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4570ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_link = urllib.parse.urljoin(\"https://s.weibo.com\", href)\n",
    "params = urllib.parse.parse_qs(urllib.parse.urlparse(full_link).query)\n",
    "q = params.get(\"q\", [None])[0]\n",
    "if q:\n",
    "    q_enc = urllib.parse.quote(q, safe=\"\")\n",
    "    detail_url = f\"https://m.s.weibo.com/vtopic/detail_new?click_from=searchpc&q={q_enc}\"\n",
    "else:\n",
    "    detail_url = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd1e422d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://m.s.weibo.com/vtopic/detail_new?click_from=searchpc&q=%23%E4%B8%AD%E5%BE%B7%E4%B8%A4%E5%9B%BD%E7%90%86%E5%BD%93%E6%90%BA%E6%89%8B%E6%8A%B5%E5%BE%A1%E7%96%BE%E9%A3%8E%E9%AA%A4%E9%9B%A8%23'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detail_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "194da588",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32md:\\python310\\lib\\site-packages\\requests\\models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32md:\\python310\\lib\\site-packages\\simplejson\\__init__.py:514\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, allow_nan, **kw)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    511\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    512\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_decimal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\python310\\lib\\site-packages\\simplejson\\decoder.py:386\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[0;32m    385\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(s, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[1;32m--> 386\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32md:\\python310\\lib\\site-packages\\simplejson\\decoder.py:416\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[0;32m    415\u001b[0m         idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetail_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHEADERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m resp\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mapparent_encoding\n\u001b[0;32m      3\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(resp\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\python310\\lib\\site-packages\\requests\\models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "resp = requests.get(detail_url, headers=HEADERS, timeout=10).json()\n",
    "resp.encoding = resp.apparent_encoding\n",
    "soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "reading = discussion = interaction = original = None\n",
    "# 3. 遍历所有 .item-col，按 .des 文本分类\n",
    "for item in soup.find_all(\"div\", class_=\"item-col\"):\n",
    "    label = item.find(\"div\", class_=\"des\").get_text(strip=True)\n",
    "    # num 中可能包含 <span> 万，需要拼接所有文本\n",
    "    num = \"\".join(item.find(\"div\", class_=\"num\").stripped_strings)\n",
    "    if label == \"阅读量\":\n",
    "        reading = num\n",
    "    elif label == \"讨论量\":\n",
    "        discussion = num\n",
    "    elif label == \"互动量\":\n",
    "        interaction = num\n",
    "    elif label == \"原创量\":\n",
    "        original = num\n",
    "\n",
    "# 4. 打印结果\n",
    "print(\"阅读量：\", reading)        # 460.7万\n",
    "print(\"讨论量：\", discussion)      # 512\n",
    "print(\"互动量：\", interaction)      # 2094\n",
    "print(\"原创量：\", original)        # 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf637a5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32md:\\python310\\lib\\site-packages\\requests\\models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32md:\\python310\\lib\\site-packages\\simplejson\\__init__.py:514\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, allow_nan, **kw)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    511\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    512\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_decimal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\python310\\lib\\site-packages\\simplejson\\decoder.py:386\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[0;32m    385\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(s, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[1;32m--> 386\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32md:\\python310\\lib\\site-packages\\simplejson\\decoder.py:416\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[0;32m    415\u001b[0m         idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocialevent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m)}\n\u001b[0;32m      4\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCookie\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# 如有需要\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# data[\"data\"][\"socialevent\"] 就是一列表\u001b[39;00m\n\u001b[0;32m      7\u001b[0m data\n",
      "File \u001b[1;32md:\\python310\\lib\\site-packages\\requests\\models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests, time\n",
    "url = \"https://m.s.weibo.com/vtopic/detail_new?click_from=searchpc&q=%23%E4%B8%AD%E5%BE%B7%E4%B8%A4%E5%9B%BD%E7%90%86%E5%BD%93%E6%90%BA%E6%89%8B%E6%8A%B5%E5%BE%A1%E7%96%BE%E9%A3%8E%E9%AA%A4%E9%9B%A8%23\"\n",
    "params = {\"cate\": \"socialevent\", \"_\": int(time.time()*1000)}\n",
    "headers = {\"User-Agent\": \"...\", \"Cookie\": \"...\"}  # 如有需要\n",
    "data = requests.get(url, params=params, headers=headers).json()\n",
    "# data[\"data\"][\"socialevent\"] 就是一列表\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
